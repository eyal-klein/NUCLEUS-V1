# Micro-Prompts Engine

**The Nervous System of NUCLEUS**

**Type**: Cloud Run Job  
**Deployment**: Automated via GitHub Actions  
**Status**: âœ… Production Ready

---

## Overview

The Micro-Prompts Engine is the nervous system of NUCLEUS. It takes the Master Prompt (the Entity's core identity) and adapts it for each agent's specific role, ensuring all agents share the same foundational identity while specializing for their unique purposes.

## Purpose

In a living organism, the nervous system transmits signals from the brain to every part of the body, coordinating action. Similarly, the Micro-Prompts Engine transmits the Entity's identity (Master Prompt) to every agent, ensuring coherent behavior across the entire system.

## Architecture

```
Entity.master_prompt (core identity)
    â†“
Micro-Prompts Engine (this job)
    â†“
Agent.system_prompt (per agent, role-specific)
```

## How It Works

### Input: Master Prompt

The Master Prompt is a comprehensive (800-1200 words) description of the Entity's core identity, generated by the Master Prompt Engine. It includes:

1. **Identity**: Who is this Entity?
2. **Communication Style**: How does this Entity communicate?
3. **Decision-Making**: How does this Entity make decisions?
4. **Goals & Direction**: What does this Entity want to achieve?
5. **Context & Relationships**: Important context
6. **Guiding Principles**: 5-7 core non-negotiable principles

### Process: LLM Adaptation

For each agent, the Micro-Prompts Engine:

1. Reads the Entity's Master Prompt
2. Reads the agent's role and purpose
3. Uses GPT-4.1-mini to adapt the Master Prompt for the agent's specific role
4. Generates a customized `system_prompt` (400-600 words)
5. Updates the agent's `system_prompt` in the database

### Output: Agent System Prompts

Each agent receives a customized system prompt that:

- **Inherits** the Entity's core identity from the Master Prompt
- **Specializes** for the agent's specific role and responsibilities
- **Maintains** consistency with all other agents in the system

## Example

### Master Prompt (Entity)

```
You are the digital extension of Eyal Klein, a visionary entrepreneur...
Your communication style is direct, authentic, and deeply thoughtful...
You make decisions based on long-term impact, not short-term gains...
```

### Agent System Prompt (Research Agent)

```
You are a research agent serving Eyal Klein, a visionary entrepreneur...
Your role is to conduct deep research on emerging technologies...
You communicate findings with clarity and depth, reflecting Eyal's thoughtful approach...
You prioritize research that aligns with long-term strategic goals...
```

## LLM Prompt

The Micro-Prompts Engine uses the following prompt structure:

```
You are adapting a Master Prompt for a specific agent role.

MASTER PROMPT (Entity Identity):
{master_prompt}

AGENT TO CUSTOMIZE:
- Name: {agent_name}
- Type: {agent_type}
- Purpose: {agent_purpose}

TASK:
Generate a customized system prompt for this agent that:
1. INHERITS the core identity from the Master Prompt
2. SPECIALIZES for the agent's specific role
3. MAINTAINS consistency with the Entity's values and style
4. Is 400-600 words, clear and actionable

OUTPUT FORMAT:
Return ONLY the customized system prompt, no explanations.
```

## Usage

### Manual Execution

```bash
gcloud run jobs execute micro-prompts \
  --region us-central1 \
  --set-env-vars ENTITY_ID=<uuid>
```

### Automated Execution

The Micro-Prompts Engine runs automatically after:

1. **Master Prompt Engine** completes (when Master Prompt is updated)
2. **New Agent Creation** (when a new agent is spawned)

## Database Schema

### Entity Table (Input)

```sql
SELECT master_prompt FROM dna.entity WHERE id = <entity_id>;
```

### Agent Table (Output)

```sql
UPDATE assembly.agents
SET system_prompt = <customized_prompt>,
    updated_at = NOW()
WHERE id = <agent_id>;
```

## Deployment

### Automated Deployment

Every push to `main` that changes files in `backend/jobs/micro-prompts/` triggers an automatic deployment via GitHub Actions.

**Workflow**: `.github/workflows/deploy-micro-prompts.yml`

### Manual Deployment

```bash
gcloud run jobs deploy micro-prompts \
  --source backend/jobs/micro-prompts \
  --region us-central1 \
  --set-env-vars DATABASE_URL=$DATABASE_URL,OPENAI_API_KEY=$OPENAI_API_KEY
```

## Configuration

### Environment Variables

- `DATABASE_URL`: PostgreSQL connection string
- `OPENAI_API_KEY`: OpenAI API key for GPT-4.1-mini
- `ENTITY_ID`: UUID of the Entity to process

### LLM Configuration

- **Model**: GPT-4.1-mini
- **Temperature**: 0.4 (balanced creativity and consistency)
- **Max Tokens**: 800 (400-600 words)

### Resource Limits

- **Memory**: 512Mi
- **CPU**: 1
- **Timeout**: 10m
- **Max Retries**: 3

## Monitoring

### Logs

```bash
gcloud logging read "resource.type=cloud_run_job AND resource.labels.job_name=micro-prompts" --limit=100
```

### Metrics

- **Execution Time**: How long it takes to update all agents
- **Agents Updated**: Number of agents processed per execution
- **LLM API Calls**: Number of calls to GPT-4.1-mini
- **Success Rate**: Percentage of successful updates

## Integration

### With Master Prompt Engine

The Micro-Prompts Engine runs after the Master Prompt Engine:

```
Master Prompt Engine â†’ Entity.master_prompt updated
    â†“
Micro-Prompts Engine â†’ All Agent.system_prompt updated
```

### With Agent Factory

When a new agent is spawned, the Micro-Prompts Engine generates its system prompt:

```
Agent Factory â†’ New agent created
    â†“
Micro-Prompts Engine â†’ Agent.system_prompt generated
```

## Benefits

### Consistency

All agents share the same foundational identity, ensuring coherent behavior across the entire system.

### Efficiency

Update the Master Prompt once, and all agents inherit the changes automatically.

### Clarity

Clear separation between Entity identity (Master Prompt) and Agent role (System Prompt).

### Scalability

Easy to add new agents with consistent behaviorâ€”just run the Micro-Prompts Engine.

## Troubleshooting

### Issue: Agents not updated

**Cause**: Master Prompt is empty or missing.

**Solution**: Run the Master Prompt Engine first to generate the Master Prompt.

### Issue: LLM API errors

**Cause**: Invalid OpenAI API key or rate limit exceeded.

**Solution**: Check `OPENAI_API_KEY` and ensure you have sufficient quota.

### Issue: Inconsistent prompts

**Cause**: LLM temperature too high.

**Solution**: Reduce temperature in LLM configuration (currently 0.4).

## Future Enhancements

- **A/B Testing**: Test different prompt variations for each agent.
- **Prompt Versioning**: Track changes to agent prompts over time.
- **Automatic Optimization**: Use feedback to improve prompts automatically.
- **Multi-Language Support**: Generate prompts in multiple languages.

---

**The nervous system is operational. The organism is coordinated.** ðŸ§¬
